---
import { Icon } from "astro-icon/components";
import { models } from "./data";
---

<section class="max-w-4xl mx-auto px-4 py-12 space-y-16">
    <article class="prose prose-lg dark:prose-invert mx-auto text-center mb-16">
        <h2
            class="text-4xl font-extrabold text-transparent bg-clip-text bg-gradient-to-r from-indigo-600 to-purple-600 dark:from-indigo-400 dark:to-purple-400 mb-6"
        >
            Guía Definitiva de Precios API LLM (2025)
        </h2>
        <p class="text-xl leading-relaxed text-slate-600 dark:text-slate-300">
            Navegar por el ecosistema de precios de la Inteligencia Artificial se ha vuelto
            complejo. Con la llegada de la serie <strong>GPT-5</strong>, los modelos <strong
                >Gemini 3</strong
            > y la familia <strong>Claude 4.5</strong>, elegir el modelo adecuado no es solo
            cuestión de inteligencia, sino de viabilidad económica. Esta calculadora te ofrece
            transparencia total para que tomes decisiones informadas.
        </p>
    </article>

    
    <div class="grid grid-cols-1 md:grid-cols-3 gap-8">
        <div
            class="group bg-white dark:bg-slate-800 p-8 rounded-2xl shadow-sm border border-slate-100 dark:border-slate-700 hover:shadow-md transition-shadow"
        >
            <div
                class="w-14 h-14 bg-indigo-100 dark:bg-indigo-900/40 rounded-xl flex items-center justify-center mb-6 text-indigo-600 dark:text-indigo-400 group-hover:scale-110 transition-transform"
            >
                <Icon name="mdi:calculator-variant" size={32} />
            </div>
            <h3 class="text-xl font-bold text-slate-900 dark:text-white mb-3">
                La Economía del Token
            </h3>
            <p class="text-slate-600 dark:text-slate-400 leading-relaxed">
                El precio no se basa en peticiones, sino en volumen. <strong>1,000 tokens</strong> equivalen
                aproximadamente a 750 palabras en inglés. En español, debido a la estructura del idioma,
                la eficiencia es ligeramente menor. Entender esto es vital para proyectar costes a escala.
            </p>
        </div>

        <div
            class="group bg-white dark:bg-slate-800 p-8 rounded-2xl shadow-sm border border-slate-100 dark:border-slate-700 hover:shadow-md transition-shadow"
        >
            <div
                class="w-14 h-14 bg-pink-100 dark:bg-pink-900/40 rounded-xl flex items-center justify-center mb-6 text-pink-600 dark:text-pink-400 group-hover:scale-110 transition-transform"
            >
                <Icon name="mdi:logout" size={32} />
            </div>
            <h3 class="text-xl font-bold text-slate-900 dark:text-white mb-3">Input vs Output</h3>
            <p class="text-slate-600 dark:text-slate-400 leading-relaxed">
                La generación (Output) es computacionalmente más costosa que la lectura (Input). Los
                proveedores suelen cobrar entre <strong>3x y 5x más</strong> por los tokens generados.
                Diseñar prompts concisos y limitar la longitud de la respuesta son estrategias clave
                de ahorro.
            </p>
        </div>

        <div
            class="group bg-white dark:bg-slate-800 p-8 rounded-2xl shadow-sm border border-slate-100 dark:border-slate-700 hover:shadow-md transition-shadow"
        >
            <div
                class="w-14 h-14 bg-amber-100 dark:bg-amber-900/40 rounded-xl flex items-center justify-center mb-6 text-amber-600 dark:text-amber-400 group-hover:scale-110 transition-transform"
            >
                <Icon name="mdi:scale-balance" size={32} />
            </div>
            <h3 class="text-xl font-bold text-slate-900 dark:text-white mb-3">
                Ratio Calidad/Precio
            </h3>
            <p class="text-slate-600 dark:text-slate-400 leading-relaxed">
                No siempre necesitas el modelo más potente. <strong>GPT-5 Mini</strong> o <strong
                    >Gemini 3 Flash</strong
                >
                ofrecen capacidades de razonamiento superiores a los modelos "flagship" de hace un año,
                pero a una fracción del coste (hasta 50x más baratos).
            </p>
        </div>
    </div>

    
    <div class="space-y-6">
        <h3 class="text-2xl font-bold text-slate-900 dark:text-white px-2">
            Tabla Comparativa de Precios (Actualizado 2025)
        </h3>
        <div
            class="overflow-hidden rounded-2xl border border-slate-200 dark:border-slate-700 shadow-sm"
        >
            <table class="w-full text-sm text-left text-slate-500 dark:text-slate-400">
                <thead
                    class="text-xs text-slate-700 uppercase bg-slate-50 dark:bg-slate-800 dark:text-slate-300"
                >
                    <tr>
                        <th scope="col" class="px-6 py-4">Modelo</th>
                        <th scope="col" class="px-6 py-4">Input (1M)</th>
                        <th scope="col" class="px-6 py-4">Output (1M)</th>
                        <th scope="col" class="px-6 py-4 hidden md:table-cell"
                            >Descripción y Uso Ideal</th
                        >
                    </tr>
                </thead>
                <tbody class="divide-y divide-slate-200 dark:divide-slate-700">
                    {
                        models.map((model) => (
                            <tr class="bg-white dark:bg-slate-900 hover:bg-slate-50 dark:hover:bg-slate-800 transition-colors">
                                <td class="px-6 py-4">
                                    <div class="flex flex-col">
                                        <span class="font-bold text-slate-900 dark:text-white text-base">
                                            {model.name}
                                        </span>
                                        <span class="text-xs font-medium text-slate-400 bg-slate-100 dark:bg-slate-800 w-fit px-2 py-0.5 rounded-full mt-1">
                                            {model.group}
                                        </span>
                                    </div>
                                </td>
                                <td class="px-6 py-4 font-mono text-slate-700 dark:text-slate-300">
                                    ${model.input.toFixed(2)}
                                </td>
                                <td class="px-6 py-4 font-mono text-slate-700 dark:text-slate-300">
                                    ${model.output.toFixed(2)}
                                </td>
                                <td class="px-6 py-4 hidden md:table-cell text-slate-600 dark:text-slate-400 leading-snug">
                                    {model.description}
                                </td>
                            </tr>
                        ))
                    }
                </tbody>
            </table>
        </div>
    </div>

    
    <article class="prose prose-lg dark:prose-invert mx-auto mt-24 max-w-none">
        <h3
            class="text-3xl font-bold text-transparent bg-clip-text bg-gradient-to-r from-indigo-600 to-purple-600 dark:from-indigo-400 dark:to-purple-400 mb-8 text-center not-prose"
        >
            Estrategias Maestras para la Optimización de Costes LLM en 2025
        </h3>
        <p
            class="text-lg text-slate-600 dark:text-slate-300 mb-12 leading-relaxed max-w-3xl mx-auto text-center"
        >
            La democratización de la Inteligencia Artificial ha traído consigo un desafío financiero
            invisible: la "inflación del token". A medida que integramos modelos de lenguaje en
            flujos de trabajo críticos, la factura de la API puede escalar silenciosamente hasta
            convertirse en el mayor coste operativo de una startup tecnológica. En 2025, la
            ingeniería de costes es tan vital como la ingeniería de prompts. A continuación,
            desglosamos las estrategias avanzadas que los CTOs están utilizando para reducir su
            gasto en IA en hasta un 70% sin sacrificar la calidad.
        </p>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-12 not-prose">
            
            <div
                class="bg-white dark:bg-slate-800/50 p-8 rounded-2xl border border-slate-100 dark:border-slate-700/50"
            >
                <h4
                    class="text-xl font-bold text-slate-900 dark:text-white mb-4 flex items-center gap-3"
                >
                    <span
                        class="flex items-center justify-center w-8 h-8 rounded-full bg-indigo-100 dark:bg-indigo-900/50 text-indigo-600 dark:text-indigo-400 text-sm font-bold"
                        >1</span
                    >
                    La Revolución de los Small Language Models (SLM)
                </h4>
                <p class="text-slate-600 dark:text-slate-400 leading-relaxed text-sm">
                    El enfoque de "un modelo para todo" ha muerto. En lugar de utilizar GPT-5 o
                    Claude Opus para clasificar correos electrónicos o extraer entidades JSON, la
                    industria se está moviendo hacia modelos especializados y pequeños. Modelos como <strong
                        >Phi-4</strong
                    >, <strong>Gemma 2</strong> o versiones cuantizadas de Llama 3 pueden ejecutarse
                    con una latencia mínima y un coste despreciable. La estrategia ganadora es la <strong
                        >arquitectura de cascada</strong
                    >: intentar resolver la tarea con un modelo barato primero, y solo escalar al
                    modelo "Pro" si la confianza de la respuesta es baja o la tarea requiere un
                    razonamiento complejo de múltiples pasos.
                </p>
            </div>

            
            <div
                class="bg-white dark:bg-slate-800/50 p-8 rounded-2xl border border-slate-100 dark:border-slate-700/50"
            >
                <h4
                    class="text-xl font-bold text-slate-900 dark:text-white mb-4 flex items-center gap-3"
                >
                    <span
                        class="flex items-center justify-center w-8 h-8 rounded-full bg-pink-100 dark:bg-pink-900/50 text-pink-600 dark:text-pink-400 text-sm font-bold"
                        >2</span
                    >
                    Leveraging Context Caching (Caché de Contexto)
                </h4>
                <p class="text-slate-600 dark:text-slate-400 leading-relaxed text-sm">
                    El "Context Caching" es la característica más disruptiva económicamente de este
                    año. Anteriormente, si tenías un manual técnico de 500 páginas y querías hacer
                    preguntas sobre él, pagabas por procesar esas 500 páginas en cada pregunta
                    individual. Con el caching (disponible en Gemini y Anthropic), ahora puedes
                    "pre-calentar" ese contexto una sola vez. Las llamadas subsiguientes solo pagan
                    por el coste del prompt nuevo, reduciendo el coste de entrada en un 90% para
                    sesiones largas de chat o análisis de documentos recurrentes. Es obligatorio
                    para aplicaciones RAG (Retrieval Augmented Generation) de alto volumen.
                </p>
            </div>

            
            <div
                class="bg-white dark:bg-slate-800/50 p-8 rounded-2xl border border-slate-100 dark:border-slate-700/50"
            >
                <h4
                    class="text-xl font-bold text-slate-900 dark:text-white mb-4 flex items-center gap-3"
                >
                    <span
                        class="flex items-center justify-center w-8 h-8 rounded-full bg-amber-100 dark:bg-amber-900/50 text-amber-600 dark:text-amber-400 text-sm font-bold"
                        >3</span
                    >
                    La Trampa de la Ventana de Contexto Infinita
                </h4>
                <p class="text-slate-600 dark:text-slate-400 leading-relaxed text-sm">
                    Ver "2 millones de tokens de contexto" es tentador. Sugiere que puedes dejar de
                    preocuparte por la recuperación de información y simplemente enviar toda tu base
                    de datos al prompt. Financieramente, esto es un suicidio. Procesar 1 millón de
                    tokens en GPT-4o cuesta aproximadamente $5. Si tienes 100 usuarios al día
                    haciendo esto, tu coste mensual supera los $15,000. La búsqueda semántica
                    (Vector Search) y los sistemas RAG siguen siendo 100x más eficientes: recuperan
                    solo los 3-5 fragmentos relevantes (apenas 2k-3k tokens) para responder a la
                    pregunta, manteniendo el coste por interacción en centavos, no en dólares.
                </p>
            </div>

            
            <div
                class="bg-white dark:bg-slate-800/50 p-8 rounded-2xl border border-slate-100 dark:border-slate-700/50"
            >
                <h4
                    class="text-xl font-bold text-slate-900 dark:text-white mb-4 flex items-center gap-3"
                >
                    <span
                        class="flex items-center justify-center w-8 h-8 rounded-full bg-emerald-100 dark:bg-emerald-900/50 text-emerald-600 dark:text-emerald-400 text-sm font-bold"
                        >4</span
                    >
                    Inferencia Open Source en Hardware Especializado
                </h4>
                <p class="text-slate-600 dark:text-slate-400 leading-relaxed text-sm">
                    La brecha de calidad entre los modelos cerrados y abiertos se ha cerrado
                    drásticamente. Mixtral 8x22B o Llama 3 70B ofrecen un rendimiento de nivel GPT-4
                    para muchas tareas. Proveedores como <strong>Groq</strong>, que utilizan LPUs
                    (Language Processing Units) en lugar de GPUs tradicionales, ofrecen estos
                    modelos a precios que hacen que la API de OpenAI parezca un artículo de lujo.
                    Para tareas de transformación de texto, resumen o generación de contenido SEO a
                    gran escala, mover la carga de trabajo a un proveedor de inferencia Open Source
                    puede reducir la factura mensual en un orden de magnitud completo.
                </p>
            </div>

            
            <div
                class="bg-white dark:bg-slate-800/50 p-8 rounded-2xl border border-slate-100 dark:border-slate-700/50"
            >
                <h4
                    class="text-xl font-bold text-slate-900 dark:text-white mb-4 flex items-center gap-3"
                >
                    <span
                        class="flex items-center justify-center w-8 h-8 rounded-full bg-blue-100 dark:bg-blue-900/50 text-blue-600 dark:text-blue-400 text-sm font-bold"
                        >5</span
                    >
                    Observabilidad y FinOps para AI
                </h4>
                <p class="text-slate-600 dark:text-slate-400 leading-relaxed text-sm">
                    No se puede optimizar lo que no se mide. Implementar herramientas de
                    observabilidad como LangSmith, Helicone o Arize AI es fundamental. Estas
                    herramientas permiten rastrear el coste por usuario, por funcionalidad o por
                    traza de ejecución. A menudo, se descubre que el 80% del coste proviene de un
                    "prompt loop" mal optimizado o de un agente autónomo que entra en bucles de
                    razonamiento innecesarios. Establecer alertas de presupuesto y límites de tokens
                    (rate limits) a nivel de aplicación es el primer cortafuegos contra sorpresas
                    desagradables en la factura de fin de mes.
                </p>
            </div>

            
            <div
                class="bg-white dark:bg-slate-800/50 p-8 rounded-2xl border border-slate-100 dark:border-slate-700/50"
            >
                <h4
                    class="text-xl font-bold text-slate-900 dark:text-white mb-4 flex items-center gap-3"
                >
                    <span
                        class="flex items-center justify-center w-8 h-8 rounded-full bg-purple-100 dark:bg-purple-900/50 text-purple-600 dark:text-purple-400 text-sm font-bold"
                        >6</span
                    >
                    Fine-Tuning Específico vs Prompting Generalista
                </h4>
                <p class="text-slate-600 dark:text-slate-400 leading-relaxed text-sm">
                    A menudo usamos prompts kilométricos llenos de ejemplos (Few-Shot Prompting)
                    para conseguir que el modelo siga un formato específico. Esto infla el coste de
                    entrada en cada llamada. Un modelo más pequeño (como GPT-4o Mini) a menudo puede
                    superar a un modelo grande si se le hace un <strong>Fine-Tuning</strong> (ajuste
                    fino) con 50-100 ejemplos de alta calidad. El Fine-Tuning permite eliminar casi todas
                    las instrucciones del prompt, reduciendo drásticamente los tokens de entrada y mejorando
                    la consistencia de la salida, logrando un doble ahorro: modelo más barato y menos
                    tokens consumidos.
                </p>
            </div>
        </div>
    </article>
</section>
